{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sleep_Stage_classification_Mendeley.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ex6c1ysfEjC9"
      },
      "source": [
        "# Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehY-8R5eKSzi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d2d7d5a-b5ea-465b-e909-fe446254804c"
      },
      "source": [
        "!pip install mne\r\n",
        "!apt-get install libfftw3-dev\r\n",
        "!pip install stockwell"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting mne\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/17/a8/7d8a10345082d4807907a268016b52dfa869b0c412cd84aa1d1de86e1e39/mne-0.22.0-py3-none-any.whl (6.9MB)\n",
            "\u001b[K     |████████████████████████████████| 6.9MB 5.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.17.1 in /usr/local/lib/python3.6/dist-packages (from mne) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from mne) (1.19.5)\n",
            "Installing collected packages: mne\n",
            "Successfully installed mne-0.22.0\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libfftw3-bin libfftw3-long3 libfftw3-quad3 libfftw3-single3\n",
            "Suggested packages:\n",
            "  libfftw3-doc\n",
            "The following NEW packages will be installed:\n",
            "  libfftw3-bin libfftw3-dev libfftw3-long3 libfftw3-quad3 libfftw3-single3\n",
            "0 upgraded, 5 newly installed, 0 to remove and 15 not upgraded.\n",
            "Need to get 3,766 kB of archives.\n",
            "After this operation, 21.2 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 libfftw3-long3 amd64 3.3.7-1 [308 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 libfftw3-quad3 amd64 3.3.7-1 [552 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/main amd64 libfftw3-single3 amd64 3.3.7-1 [764 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/main amd64 libfftw3-bin amd64 3.3.7-1 [32.7 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/main amd64 libfftw3-dev amd64 3.3.7-1 [2,108 kB]\n",
            "Fetched 3,766 kB in 1s (3,628 kB/s)\n",
            "Selecting previously unselected package libfftw3-long3:amd64.\n",
            "(Reading database ... 146442 files and directories currently installed.)\n",
            "Preparing to unpack .../libfftw3-long3_3.3.7-1_amd64.deb ...\n",
            "Unpacking libfftw3-long3:amd64 (3.3.7-1) ...\n",
            "Selecting previously unselected package libfftw3-quad3:amd64.\n",
            "Preparing to unpack .../libfftw3-quad3_3.3.7-1_amd64.deb ...\n",
            "Unpacking libfftw3-quad3:amd64 (3.3.7-1) ...\n",
            "Selecting previously unselected package libfftw3-single3:amd64.\n",
            "Preparing to unpack .../libfftw3-single3_3.3.7-1_amd64.deb ...\n",
            "Unpacking libfftw3-single3:amd64 (3.3.7-1) ...\n",
            "Selecting previously unselected package libfftw3-bin.\n",
            "Preparing to unpack .../libfftw3-bin_3.3.7-1_amd64.deb ...\n",
            "Unpacking libfftw3-bin (3.3.7-1) ...\n",
            "Selecting previously unselected package libfftw3-dev:amd64.\n",
            "Preparing to unpack .../libfftw3-dev_3.3.7-1_amd64.deb ...\n",
            "Unpacking libfftw3-dev:amd64 (3.3.7-1) ...\n",
            "Setting up libfftw3-quad3:amd64 (3.3.7-1) ...\n",
            "Setting up libfftw3-single3:amd64 (3.3.7-1) ...\n",
            "Setting up libfftw3-long3:amd64 (3.3.7-1) ...\n",
            "Setting up libfftw3-bin (3.3.7-1) ...\n",
            "Setting up libfftw3-dev:amd64 (3.3.7-1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Collecting stockwell\n",
            "  Downloading https://files.pythonhosted.org/packages/d0/2b/a8b085b85c58a8744b9ca1148f4dd9473f3faf7e4cd3b478555f8444c6f7/stockwell-0.1.0.tar.gz\n",
            "Building wheels for collected packages: stockwell\n",
            "  Building wheel for stockwell (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for stockwell: filename=stockwell-0.1.0-cp36-cp36m-linux_x86_64.whl size=30825 sha256=19014c1d61955b1e44621c44562ec64a1d8c512e19ac62837efc337e7f8b1af4\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/41/fb/f42cf38abb80b174bd36e3e25e178c2a3a4a5148ef28c86180\n",
            "Successfully built stockwell\n",
            "Installing collected packages: stockwell\n",
            "Successfully installed stockwell-0.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xeCR4FSKHI1"
      },
      "source": [
        "import sys\n",
        "import os\n",
        "import numpy as np\n",
        "import mne\n",
        "from mne.preprocessing import ICA\n",
        "import warnings\n",
        "import stockwell\n",
        "import stockwell.smt as smt\n",
        "import matplotlib.pyplot as plt\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lF6nGlJ0KXKX"
      },
      "source": [
        "def load_eeg_raw(subj_type, id):\n",
        "    subject_folder = f'/content/drive/MyDrive/Mendeley_Database/{subj_type}_Subjects'\n",
        "    subject_raw_file = os.path.join(subject_folder, f'{subj_type}_EDF', f'{subj_type}_Subject_{id}.edf')\n",
        "    raw = mne.io.read_raw_edf(subject_raw_file, preload=True)\n",
        "    return raw"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txZHiI6CmIq9"
      },
      "source": [
        "# Preprocessing Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C59W4yApQvcq"
      },
      "source": [
        "def fit_ica(raw, exclude, crop_time):\n",
        "    raw.crop(tmax=crop_time)\n",
        "    ica = ICA(n_components=0.9999, random_state=960, max_iter=200)\n",
        "    ica.fit(raw)\n",
        "    return raw, ica\n",
        "\n",
        "def plot_ica_sources(raw, ica):\n",
        "    ica.plot_sources(raw, show_scrollbars=False)\n",
        "    del raw\n",
        "\n",
        "def apply_ica(raw, ica):\n",
        "    ica.apply(raw)\n",
        "    return raw"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEA-Mkctmq3N"
      },
      "source": [
        "# Annotating, transforming and plotting data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-_WDtL8zSh0"
      },
      "source": [
        "def get_sleep_profile(subj_type, id):\n",
        "    \"\"\"Return sleep profile and its reliability\"\"\"    \n",
        "    sleep_profile = []\n",
        "    subject_folder = f'/content/drive/MyDrive/Mendeley_Database/{subj_type}_Subjects'\n",
        "    subject_profile_path = os.path.join(subject_folder, f'{subj_type}_Outputs', \n",
        "                                         f'PSG_Output_{subj_type}_Subject_{id}', 'Sleep Profile.txt')\n",
        "    subject_profile_path_rel = os.path.join(subject_folder, f'{subj_type}_Outputs', \n",
        "                                         f'PSG_Output_{subj_type}_Subject_{id}', 'Sleep Profile Reliability.txt')\n",
        "    \n",
        "    with open(subject_profile_path, 'rt') as profile, open(subject_profile_path_rel, 'rt') as profile_rel:\n",
        "        for line_p, line_pr in zip(list(profile)[7:], list(profile_rel)[6:]):\n",
        "            s_profile = []\n",
        "            if (line_p.find(';') != -1 and line_pr.find(';') != -1):\n",
        "                # Sleep profile\n",
        "                line_p = line_p.rstrip('\\n').split(';')\n",
        "                line_p[1] = line_p[1].lstrip(' ')\n",
        "                s_profile.append(line_p[1])\n",
        "                # Sleep profile reliability\n",
        "                line_pr = line_pr.rstrip('\\n').split(';')\n",
        "                line_pr[1] = line_pr[1].lstrip(' ')\n",
        "                s_profile.append(line_pr[1].split(' ')[0])\n",
        "        \n",
        "            sleep_profile.append(s_profile)\n",
        "    return sleep_profile"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sY-iDxo-PBt4"
      },
      "source": [
        "def get_sleep_profile_2(subj_type, id):\n",
        "    \"\"\"Return sleep profile and its reliability\"\"\"    \n",
        "    sleep_profile = []\n",
        "    subject_folder = f'/content/drive/MyDrive/Mendeley_Database/{subj_type}_Subjects'\n",
        "\n",
        "    subject_profile_path = os.path.join(subject_folder, \n",
        "                                        f'{subj_type}_Outputs', \n",
        "                                        f'PSG_Output_{subj_type}_Subject_{id}', \n",
        "                                        'Sleep Profile.txt')\n",
        "    \n",
        "    subject_profile_path_rel = os.path.join(subject_folder,\n",
        "                                            f'{subj_type}_Outputs', \n",
        "                                            f'PSG_Output_{subj_type}_Subject_{id}', \n",
        "                                            'Sleep Profile Reliability.txt')\n",
        "    \n",
        "    with open(subject_profile_path, 'rt') as profile, open(subject_profile_path_rel, 'rt') as profile_rel:\n",
        "        # Sleep profile\n",
        "        for line in list(profile):\n",
        "            if (line.find(';') != -1):\n",
        "                line = line.rstrip('\\n').split(';')\n",
        "                line[1] = line[1].lstrip(' ')\n",
        "                sleep_profile.append([line[1]])\n",
        "        \n",
        "        # Sleep profile reliability\n",
        "        index = 0\n",
        "        for line in list(profile_rel):\n",
        "            if (line.find(';') != -1):\n",
        "                line = line.rstrip('\\n').split(';')\n",
        "                line[1] = line[1].lstrip(' ')\n",
        "                if (index < len(sleep_profile)):\n",
        "                    sleep_profile[index].append(line[1])\n",
        "                index += 1\n",
        "        \n",
        "        return sleep_profile"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnQQF_7FzR7l"
      },
      "source": [
        "def set_annotations_to_raw(raw, sleep_profile):\n",
        "    \"\"\"Read annotations from txt file and set it to raw data\"\"\"\n",
        "    sleep_profile = np.array(sleep_profile)\n",
        "    onset = np.arange(start=0, stop=30*len(sleep_profile), step=30)\n",
        "    durations = [30] * len(sleep_profile)\n",
        "    descriptions = sleep_profile[:, 0]\n",
        "    normal_subject_01_annot = mne.Annotations(onset, durations, \n",
        "                                              descriptions, \n",
        "                                              orig_time=raw.info['meas_date'])\n",
        "    raw.set_annotations(normal_subject_01_annot)\n",
        "    return raw"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEh6XVsdB5ls"
      },
      "source": [
        "def plotspec(psx, fs, lofreq=None, hifreq=None, t1=None, t2=None):\n",
        "    \"\"\"Modify dimensions and properties of stockwell transformed image\"\"\"\n",
        "    extent = [0, psx.shape[1], 0.0, fs/2.0]\n",
        "    if (t1 != None and t2 != None):\n",
        "        extent[0] = t1\n",
        "        extent[1] = t2\n",
        "    if (lofreq != None):\n",
        "        extent[2] = lofreq\n",
        "    if (hifreq != None):\n",
        "        extent[3] = hifreq\n",
        "\n",
        "    return plt.imshow(psx, extent=extent, aspect='auto', origin='lower')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxrLhZH5VoX6"
      },
      "source": [
        "def stockwell_transform(segment):\n",
        "    \"\"\"Return stockwell transformed epoch\"\"\"\n",
        "    return abs(smt.st(segment))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6xU5aWc9kDS"
      },
      "source": [
        "def plot_epochs_stockwell(epochs, sleep_profile, sleep_type=None, \n",
        "                          id=None, save=False, high_reliability=False):\n",
        "    \"\"\"Plot stockwell transformed epochs\"\"\"\n",
        "    sleep_profile = mask_profile(sleep_profile, epochs.drop_log)\n",
        "    for rel, i in zip(sleep_profile, range(len(epochs))):\n",
        "        if (rel[1] == '100%-70% Reliability'):\n",
        "            segment = epochs[i].get_data().flatten()\n",
        "            segment_st = stockwell_transform(segment)\n",
        "            plt.yscale('log')\n",
        "            fig = plotspec(segment_st, 300.0)\n",
        "            plt.ylim(0, 50) # Cropping max frequency to 45Hz\n",
        "            plt.axis('off')\n",
        "            if (save):\n",
        "                plt.savefig(f'''/content/drive/MyDrive/images/{rel[0]}/{\n",
        "                            sleep_type}_{id}_{rel[0]}_{i}.png''', \n",
        "                            bbox_inches='tight', pad_inches = 0)            \n",
        "            else:\n",
        "                plt.title(f'{rel[0]}, {rel[1]}')\n",
        "                plt.show()\n",
        "            plt.clf()\n",
        "            del segment_st \n",
        "            del fig\n",
        "            # !cat /proc/meminfo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMpkJLUywvjf"
      },
      "source": [
        "def mask_profile(sleep_profile, mask):\n",
        "    \"\"\"Drop corresponding profiles according to dropped epochs\"\"\"\n",
        "    import operator\n",
        "    mask = list(map(operator.not_, list(map(bool, mask))))\n",
        "    return np.array(sleep_profile)[mask]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24ceYpieTCcF"
      },
      "source": [
        "import time\n",
        "def plot_epochs(epochs, sleep_profile):\n",
        "    \"\"\"Plots each 30 second epoch\"\"\"\n",
        "    for rel, i in zip(sleep_profile, range(len(epochs))):\n",
        "        if not (epochs.drop_log[i]):\n",
        "            print((rel[0], rel[1]))\n",
        "            epochs[i].plot(show_scrollbars=False)\n",
        "            time.sleep(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9h1Ny6hwhs3Z"
      },
      "source": [
        "type_of_subj = ['Insomniac']\r\n",
        "for t in type_of_subj:\r\n",
        "    for id in range(6, 12):\r\n",
        "        raw = load_eeg_raw(t, id)\r\n",
        "        sleep_profile = get_sleep_profile_2(t, id)\r\n",
        "        raw = set_annotations_to_raw(raw, sleep_profile)\r\n",
        "        raw, ica = fit_ica(raw, exclude=[0], crop_time=None)\r\n",
        "        plot_ica_sources(raw, ica) \r\n",
        "        apply_ica(raw, ica)\r\n",
        "        raw.pick_channels(['O1'])\r\n",
        "        events, event_id = mne.events_from_annotations(raw)\r\n",
        "        epochs = mne.Epochs(raw, events, event_id, \r\n",
        "                            tmin=-0.1, tmax=30, preload=True)\r\n",
        "        del raw, ica, events\r\n",
        "        plot_epochs_stockwell(epochs, sleep_profile, t, id, save=True)\r\n",
        "        del epochs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6F356Ieh81D"
      },
      "source": [
        "# Splitting images, training dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ttRwyp8Ymvgk",
        "outputId": "ad34589b-5924-47a4-e96a-c5f8df4215d9"
      },
      "source": [
        "!pip install split-folders\n",
        "import splitfolders\n",
        "splitfolders.ratio('/content/drive/MyDrive/images', output=\"/content/output\", seed=4269, ratio=(.8, 0.1, 0.1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting split-folders\n",
            "  Downloading https://files.pythonhosted.org/packages/b8/5f/3c2b2f7ea5e047c8cdc3bb00ae582c5438fcdbbedcc23b3cc1c2c7aae642/split_folders-0.4.3-py3-none-any.whl\n",
            "Installing collected packages: split-folders\n",
            "Successfully installed split-folders-0.4.3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Copying files: 8069 files [1:04:34,  2.08 files/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFj4rztWngkA",
        "outputId": "4024df1c-21a2-4bfc-e1fb-6f13f2eb7824"
      },
      "source": [
        "import os\n",
        "# base_dir = '/content/output_binary/'\n",
        "# base_dir = '/content/output_ternary/'\n",
        "base_dir = '/content/output'\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "validation_dir = os.path.join(base_dir, 'val')\n",
        "test_dir = os.path.join(base_dir, 'test')\n",
        "labels_to_values = {'Stage 1': 1, 'Stage 2': 2, 'Stage 3': 3, 'Wake': 4, 'Rem': 5}\n",
        "\n",
        "labels_to_dir = {}\n",
        "for l in labels_to_values:\n",
        "    labels_to_dir[f'train_{l}_dir'] = os.path.join(train_dir, l)\n",
        "    labels_to_dir[f'val_{l}_dir'] = os.path.join(validation_dir, l)\n",
        "    labels_to_dir[f'test_{l}_dir'] = os.path.join(test_dir, l)\n",
        "print(labels_to_dir)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'train_Stage 1_dir': '/content/output/train/Stage 1', 'val_Stage 1_dir': '/content/output/val/Stage 1', 'test_Stage 1_dir': '/content/output/test/Stage 1', 'train_Stage 2_dir': '/content/output/train/Stage 2', 'val_Stage 2_dir': '/content/output/val/Stage 2', 'test_Stage 2_dir': '/content/output/test/Stage 2', 'train_Stage 3_dir': '/content/output/train/Stage 3', 'val_Stage 3_dir': '/content/output/val/Stage 3', 'test_Stage 3_dir': '/content/output/test/Stage 3', 'train_Wake_dir': '/content/output/train/Wake', 'val_Wake_dir': '/content/output/val/Wake', 'test_Wake_dir': '/content/output/test/Wake', 'train_Rem_dir': '/content/output/train/Rem', 'val_Rem_dir': '/content/output/val/Rem', 'test_Rem_dir': '/content/output/test/Rem'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYQrLVg_nn_M"
      },
      "source": [
        "for l in labels_to_values:\n",
        "    print(f'total training {l} images:', len(os.listdir(labels_to_dir[f'train_{l}_dir'])))\n",
        "    print(f'total validation {l} images:', len(os.listdir(labels_to_dir[f'val_{l}_dir'])))\n",
        "    print(f'total test {l} images:', len(os.listdir(labels_to_dir[f'test_{l}_dir'])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8klk32anFPP",
        "outputId": "d4ffe1db-b81e-4afe-df81-c87b44fe94c1"
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "train_datagen = ImageDataGenerator(\n",
        "    # shear_range=0.2,\n",
        "    # zoom_range=0.2,\n",
        "    rescale=1./255)\n",
        "val_datagen = ImageDataGenerator(\n",
        "    # shear_range=0.2,\n",
        "    # zoom_range=0.2,\n",
        "    rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    # shear_range=0.2,\n",
        "    # zoom_range=0.2)\n",
        "    )\n",
        "\n",
        "# Flow training images in batches of 32 using train_datagen generator\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_dir,  \n",
        "        class_mode='categorical',\n",
        "        classes=['Stage_1_new', 'Stage_2_new', 'Stage_3_new', 'Wake_new', 'Rem'],\n",
        "        target_size=(256, 256))\n",
        "\n",
        "# Flow validation images in batches of 32 using val_datagen generator\n",
        "validation_generator = val_datagen.flow_from_directory(\n",
        "        validation_dir,\n",
        "        class_mode='categorical',\n",
        "        classes=['Stage 1', 'Stage 2', 'Stage 3', 'Wake', 'Rem'],\n",
        "        target_size=(256, 256))\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "        test_dir,\n",
        "        shuffle=False,\n",
        "        class_mode='categorical',\n",
        "        classes=['Stage 1', 'Stage 2', 'Stage 3', 'Wake', 'Rem'],\n",
        "        target_size=(256, 256))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 5140 images belonging to 5 classes.\n",
            "Found 779 images belonging to 5 classes.\n",
            "Found 786 images belonging to 5 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mSDNazbnr0F",
        "outputId": "62dff640-1ed5-42cc-e709-10dae0161557"
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Model\n",
        "img_input = layers.Input(shape=(256, 256, 3))\n",
        "\n",
        "x = layers.Conv2D(10, 3, activation='relu')(img_input)\n",
        "x = layers.MaxPooling2D(2)(x)\n",
        "x = layers.Conv2D(20, 3, activation='relu')(x)\n",
        "x = layers.MaxPooling2D(2)(x)\n",
        "x = layers.Conv2D(30, 3, activation='relu')(x)\n",
        "x = layers.MaxPooling2D(2)(x)\n",
        "x = layers.Conv2D(40, 3, activation='relu')(x)\n",
        "x = layers.MaxPooling2D(2)(x)\n",
        "x = layers.Conv2D(50, 3, activation='relu')(x)\n",
        "x = layers.MaxPooling2D(2)(x)\n",
        "\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dense(128, activation='relu')(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "output = layers.Dense(5, activation='softmax')(x)\n",
        "\n",
        "model = Model(img_input, output)\n",
        "model.summary()\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=RMSprop(lr=0.001),\n",
        "              metrics='accuracy')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_10 (InputLayer)        [(None, 256, 256, 3)]     0         \n",
            "_________________________________________________________________\n",
            "conv2d_40 (Conv2D)           (None, 254, 254, 10)      280       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_40 (MaxPooling (None, 127, 127, 10)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_41 (Conv2D)           (None, 125, 125, 20)      1820      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_41 (MaxPooling (None, 62, 62, 20)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_42 (Conv2D)           (None, 60, 60, 30)        5430      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_42 (MaxPooling (None, 30, 30, 30)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_43 (Conv2D)           (None, 28, 28, 40)        10840     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_43 (MaxPooling (None, 14, 14, 40)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_44 (Conv2D)           (None, 12, 12, 50)        18050     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_44 (MaxPooling (None, 6, 6, 50)          0         \n",
            "_________________________________________________________________\n",
            "flatten_8 (Flatten)          (None, 1800)              0         \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 128)               230528    \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 5)                 645       \n",
            "=================================================================\n",
            "Total params: 267,593\n",
            "Trainable params: 267,593\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "goPNlLKTOpnH"
      },
      "source": [
        "!zip -r /content/images.zip /content/images/\n",
        "# from google.colab import files\n",
        "# files.download('/content/output.zip') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82mWai8kpYtg"
      },
      "source": [
        "!unzip /content/images.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8m_uAtORITE"
      },
      "source": [
        "del model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcMIFCpqnxNh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 803
        },
        "outputId": "2d9bebec-8a35-4639-ba01-744c2cd1cb4c"
      },
      "source": [
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      epochs=50,\n",
        "      validation_data=validation_generator,\n",
        "      shuffle=False,\n",
        "      verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "161/161 [==============================] - 26s 156ms/step - loss: 1.4012 - accuracy: 0.3690 - val_loss: 1.1336 - val_accuracy: 0.4647\n",
            "Epoch 2/50\n",
            "161/161 [==============================] - 25s 154ms/step - loss: 1.1212 - accuracy: 0.5189 - val_loss: 1.0966 - val_accuracy: 0.4596\n",
            "Epoch 3/50\n",
            "161/161 [==============================] - 25s 153ms/step - loss: 1.0466 - accuracy: 0.5527 - val_loss: 0.9899 - val_accuracy: 0.5687\n",
            "Epoch 4/50\n",
            "161/161 [==============================] - 25s 152ms/step - loss: 1.0022 - accuracy: 0.5793 - val_loss: 1.0215 - val_accuracy: 0.5866\n",
            "Epoch 5/50\n",
            "161/161 [==============================] - 25s 154ms/step - loss: 0.9536 - accuracy: 0.6090 - val_loss: 1.0561 - val_accuracy: 0.5173\n",
            "Epoch 6/50\n",
            "161/161 [==============================] - 25s 152ms/step - loss: 0.9017 - accuracy: 0.6279 - val_loss: 1.0111 - val_accuracy: 0.5533\n",
            "Epoch 7/50\n",
            "161/161 [==============================] - 25s 154ms/step - loss: 0.8360 - accuracy: 0.6562 - val_loss: 0.9234 - val_accuracy: 0.6162\n",
            "Epoch 8/50\n",
            "161/161 [==============================] - 25s 153ms/step - loss: 0.7806 - accuracy: 0.6923 - val_loss: 1.0746 - val_accuracy: 0.5443\n",
            "Epoch 9/50\n",
            "161/161 [==============================] - 25s 153ms/step - loss: 0.7182 - accuracy: 0.7180 - val_loss: 1.1076 - val_accuracy: 0.5789\n",
            "Epoch 10/50\n",
            "161/161 [==============================] - 25s 153ms/step - loss: 0.6338 - accuracy: 0.7536 - val_loss: 1.0321 - val_accuracy: 0.5815\n",
            "Epoch 11/50\n",
            "161/161 [==============================] - 25s 152ms/step - loss: 0.5513 - accuracy: 0.7907 - val_loss: 0.9697 - val_accuracy: 0.6123\n",
            "Epoch 12/50\n",
            " 62/161 [==========>...................] - ETA: 13s - loss: 0.4777 - accuracy: 0.8042"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-69-662e1d9d613b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m       \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m       \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m       verbose=1)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1859\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1860\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1861\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1862\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1863\u001b[0m   def evaluate_generator(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1103\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1105\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1106\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1107\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    452\u001b[0m     \"\"\"\n\u001b[1;32m    453\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    294\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unrecognized hook: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    314\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_batches_for_timing_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    354\u001b[0m       \u001b[0mhook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_supports_tf_logs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m         \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnumpy_logs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Only convert once.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1018\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1020\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1021\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1022\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1082\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m       \u001b[0;31m# Only block async when verbose = 1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1084\u001b[0;31m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1085\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1086\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36mto_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    512\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 514\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 659\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 659\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    508\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    511\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1069\u001b[0m     \"\"\"\n\u001b[1;32m   1070\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1071\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1072\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1035\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1038\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m       \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MenynXCEU3n"
      },
      "source": [
        "# Evaluation and Result Graphs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_vh6vZVpplM"
      },
      "source": [
        "#@title Stage 1 vs Stage 2 vs Stage 3 vs Wake vs Rem\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Retrieve a list of accuracy results on training and validation data\n",
        "# sets for each training epoch\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "# Retrieve a list of list results on training and validation data\n",
        "# sets for each training epoch\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "# Get number of epochs\n",
        "epochs = range(len(acc))\n",
        "\n",
        "# Plot training and validation accuracy per epoch\n",
        "plt.plot(epochs, acc)\n",
        "plt.plot(epochs, val_acc)\n",
        "plt.title('Training and validation accuracy')\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "# Plot training and validation loss per epoch\n",
        "plt.plot(epochs, loss)\n",
        "plt.plot(epochs, val_loss)\n",
        "plt.title('Training and validation loss')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfbZifvmp0ja",
        "outputId": "53ed8825-bb85-44a2-870d-880a0a2caeeb"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "model.evaluate(x=test_generator)\n",
        "y_pred = (model.predict(test_generator))\n",
        "y_pred = np.argmax(y_pred, axis=1)\n",
        "cm=confusion_matrix(test_generator.classes,y_pred)\n",
        "print(test_generator.class_indices)\n",
        "print(\"Confusion matrix is\")\n",
        "print(cm)\n",
        "for i in range(5):\n",
        "  print(\"Accuracy for class \", i)\n",
        "  print((cm.diagonal()/cm.sum(axis=1))[i] *100) #Individual Class Accuracy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25/25 [==============================] - 3s 126ms/step - loss: 1.2988 - accuracy: 0.5662\n",
            "{'Stage 1': 0, 'Stage 2': 1, 'Stage 3': 2, 'Wake': 3, 'Rem': 4}\n",
            "Confusion matrix is\n",
            "[[125  28  29  40  70]\n",
            " [ 18 102  34   3   3]\n",
            " [  4  45 101   3   3]\n",
            " [ 10   5   4 106  16]\n",
            " [ 14   2   6   4  11]]\n",
            "Accuracy for class  0\n",
            "42.80821917808219\n",
            "Accuracy for class  1\n",
            "63.74999999999999\n",
            "Accuracy for class  2\n",
            "64.74358974358975\n",
            "Accuracy for class  3\n",
            "75.177304964539\n",
            "Accuracy for class  4\n",
            "29.72972972972973\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}